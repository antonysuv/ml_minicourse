{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simulation\n",
    "### 1. Simulate trees with branch lengths \n",
    "```\n",
    "Rscript alisim_input_generate.r -d exp,0.01 -l 1000 -t '((A,B),C,D);' -n 10000 -m topo1\n",
    "Rscript alisim_input_generate.r -d exp,0.01 -l 1000 -t '((A,C),B,D);' -n 10000 -m topo2\n",
    "Rscript alisim_input_generate.r -d exp,0.01 -l 1000 -t '((A,D),B,C);' -n 10000 -m topo3\n",
    "``` \n",
    "### 2. Aggregate tree topologies \n",
    "```\n",
    "cat experiment_exp_0.01_4taxa_topo*/TRAIN/train_simulation.tre > TRAIN_all.tre\n",
    "cat experiment_exp_0.01_4taxa_topo*/TEST/test_simulation.tre > TEST_all.tre\n",
    "Rscript partition_generate.r -n 30000 -l 1000 -f TRAIN\n",
    "Rscript partition_generate.r -n 30000 -l 1000 -f TEST\n",
    "```\n",
    "\n",
    "### 3. Simulate alignments\n",
    "```\n",
    "./iqtree-2.3.2-macOS-intel/bin/iqtree2 --alisim TRAIN_all_concat -Q TRAIN.part -t TRAIN_all.tre -m JC --out-format fasta\n",
    "./iqtree-2.3.2-macOS-intel/bin/iqtree2 --alisim TEST_all_concat -Q TEST.part -t TEST_all.tre -m JC --out-format fasta\n",
    "```\n",
    "\n",
    "### 4. Split result into multiple fasta files\n",
    "```\n",
    "awk '{print$2$3$5}' TRAIN.part | tail -n +3 > TRAIN.amas_part\n",
    "python ./AMAS-master/amas/AMAS.py split -f fasta -i TRAIN_all_concat.fa -d dna -l TRAIN.amas_part -u fasta\n",
    "find . -type f -name \"TRAIN*out.fas\" | sort -V | xargs cat > TRAIN_all.fasta\n",
    "find . -type f -name \"TRAIN*out.fas\" | xargs rm\n",
    "\n",
    "awk '{print$2$3$5}' TEST.part | tail -n +3 > TEST.amas_part\n",
    "python ./AMAS-master/amas/AMAS.py split -f fasta -i TEST_all_concat.fa -d dna -l TEST.amas_part -u fasta\n",
    "find . -type f -name \"TEST*out.fas\" | sort -V | xargs cat > TEST_all.fasta\n",
    "find . -type f -name \"TEST*out.fas\" | xargs rm\n",
    "```  \n",
    "\n",
    "### 5. Convert fasta to numerical input\n",
    "```\n",
    "python fasta2numeric.py --tr TRAIN_all.fasta --te TEST_all.fasta \n",
    "```\n",
    "\n",
    "### 6. Extract site patterns\n",
    "```\n",
    "python numeric2pattern.py --tr TRAIN.npy --te TEST.npy --co 4\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine learning models\n",
    "### 1. Tree topology prediction using Random Forest (RF) algorithm\n",
    "First, we are going to train RF and evaluate its performance on our TEST dataset. The RF model takes site patterns (X) and topology class (Y) as an input.     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load numpy and machine learning module in python\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import numpy as np\n",
    "#Define Random Forest Classifier \n",
    "clf = RandomForestClassifier(max_depth=2, random_state=0)\n",
    "#Read numpy input TRAIN data X (i.e. our site patterns that we extracted from alignments)\n",
    "X_train = np.load(\"TRAIN_sitepattern_4.npy\")\n",
    "#Let's check its shape\n",
    "X_train.shape\n",
    "#Create labels Y for the input data\n",
    "Y_train = np.repeat(np.array([0,1,2]),10000)\n",
    "#Train our classifier\n",
    "clf.fit(X_train, Y_train)\n",
    "#Read numpy input TEST data X\n",
    "X_test = np.load(\"TEST_sitepattern_4.npy\")\n",
    "Y_test = Y_train\n",
    "#Let's predict labels for the TEST data and get the accuracy\n",
    "clf.score(X_test,Y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Tree topology prediction using Multilayer perceptron (MLP)\n",
    "Second, we will experiment with simple neural network arhitectures called Multilayer perceptrons or MLPs. The MLP takes site patterns (X) and topology class (Y) as an input.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import time\n",
    "import sys, argparse, os\n",
    "import numpy as np\n",
    "from itertools import product, combinations\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.layers import Input, Dense, Flatten, Dropout, BatchNormalization, ZeroPadding2D\n",
    "from tensorflow.keras.layers import Conv2D\n",
    "from tensorflow.keras.layers import MaxPooling2D, AveragePooling2D\n",
    "from tensorflow.keras.layers import concatenate\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "X_train = np.load(\"TRAIN_sitepattern_4.npy\")\n",
    "Y_train = to_categorical(np.repeat(np.array([0,1,2]),10000),num_classes=3)\n",
    "\n",
    "# Length of feature vector, i.e. number of patterns \n",
    "N_patterns=X_train.shape[1]\n",
    "    \n",
    "#Number of tree topologies\n",
    "N_labels = 3\n",
    "    \n",
    "visible_layer = Input(shape=(N_patterns,))\n",
    "hidden1 = Dense(10,activation='relu')(visible_layer)\n",
    "hidden2 = Dense(10,activation='relu')(hidden1)\n",
    "output = Dense(N_labels, activation='softmax')(hidden2)\n",
    "    \n",
    "\n",
    "model_mlp = Model(inputs=visible_layer, outputs=output)\n",
    "model_mlp.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
    "    \n",
    "#Print model\n",
    "print(model_mlp.summary())\n",
    "   \n",
    "#Model stopping criteria\n",
    "callback=EarlyStopping(monitor='val_loss', min_delta=0.0001, patience=5, verbose=1, mode='auto')\n",
    "  \n",
    "#Model training    \n",
    "model_mlp.fit(x=X_train,y=Y_train,batch_size=50,callbacks=callback,epochs=50,verbose=1,shuffle=True,validation_split=0.1)\n",
    "\n",
    "#Read numpy input TEST data X\n",
    "X_test = np.load(\"TEST_sitepattern_4.npy\")\n",
    "Y_test = Y_train\n",
    "#Let's predict labels for the TEST data and get the accuracy\n",
    "evals_class = model_mlp.evaluate(X_test,Y_test,batch_size=100, verbose=1, steps=None)\n",
    "print(evals_class[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Tree topology prediction using Convolutional Neural Nets (CNNs)\n",
    "Finally, we perform topology estimation using Convolutional Neural Nets (CNNs). The CNN takes site patterns (X) and topology class (Y) as an input. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import time\n",
    "import sys, argparse, os\n",
    "import numpy as np\n",
    "from math import factorial\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.layers import Input, Dense, Flatten, Dropout, BatchNormalization, ZeroPadding2D, Activation\n",
    "from tensorflow.keras.layers import Conv2D, Conv1D\n",
    "from tensorflow.keras.layers import MaxPooling2D, AveragePooling2D\n",
    "from tensorflow.keras.layers import concatenate\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "X_train = np.load(\"TRAIN.npy\")\n",
    "Y_train = to_categorical(np.repeat(np.array([0,1,2]),10000),num_classes=3)\n",
    "\n",
    "# Length of MSA, i.e. number of sites \n",
    "Aln_length = X_train.shape[2]\n",
    "# Number of taxa in MSA\n",
    "Ntaxa = X_train.shape[1]\n",
    "#Number of tree topologies\n",
    "N_labels = 3\n",
    "    \n",
    "    \n",
    "#1. MSA CNN branch 1 \n",
    "#Hyperparameters\n",
    "#Hight (horizontal)\n",
    "conv_x=[Ntaxa,1,1,1,1,1,1,1]\n",
    "#Width (vertical)\n",
    "conv_y=[1,2,2,2,2,2,2,2]\n",
    "pool=[1,4,4,4,2,2,2,1]\n",
    "filter_s=[10,10,10,10,10,10]\n",
    "\n",
    "conv_pool_n = 2\n",
    "# CNN Arhitecture\n",
    "visible_msa = Input(shape=X_train.shape[1:])\n",
    "x = visible_msa\n",
    "for l in list(range(0,conv_pool_n)):\n",
    "    x = ZeroPadding2D(padding=((0, 0), (0,conv_y[l]-1)))(x)\n",
    "    x = Conv2D(filters=filter_s[l], kernel_size=(conv_x[l], conv_y[l]), strides=1,activation='relu')(x)\n",
    "    x = AveragePooling2D(pool_size=(1,pool[l]))(x)  \n",
    "\n",
    "output_msa = Flatten()(x)\n",
    "  \n",
    "hidden1 = Dense(1000,activation='relu')(output_msa)\n",
    "output = Dense(N_labels, activation='linear')(hidden1)\n",
    "model_cnn = Model(inputs=visible_msa, outputs=output)\n",
    "model_cnn.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
    "    \n",
    "#Print model\n",
    "print(model_cnn.summary())\n",
    "   \n",
    "#Model stopping criteria\n",
    "callback=EarlyStopping(monitor='val_loss', min_delta=0.0001, patience=5, verbose=1, mode='auto')\n",
    "    \n",
    "model_cnn.fit(x=X_train,y=Y_train,batch_size=200,callbacks=callback,epochs=30,verbose=1,shuffle=True,validation_split=0.1)\n",
    "\n",
    "#Read numpy input TEST data X\n",
    "X_test = np.load(\"TEST.npy\")\n",
    "Y_test = Y_train\n",
    "#Let's predict labels for the TEST data and get the accuracy\n",
    "evals_class = model_cnn.evaluate(X_test,Y_test,batch_size=100, verbose=1, steps=None)\n",
    "print(evals_class[1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
